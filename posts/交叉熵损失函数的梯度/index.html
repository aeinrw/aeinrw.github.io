<!DOCTYPE html>
<html><head>
    <title>交叉熵损失函数的梯度</title>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="1. 交叉熵梯度的推导
1.1 交叉熵损失函数">
    <meta name="author" content="">


    <link rel="apple-touch-icon" href="https://blog.aeinrw.com/image/apple-touch-icon.png">
    <link rel="icon" href="https://blog.aeinrw.com/image/favicon.ico">

    <link rel="stylesheet" href="https://at.alicdn.com/t/font_1907205_qcnyqfdpzc.css">
    
    <link rel="stylesheet" href="https://blog.aeinrw.com/css/style.css">
    
    


    <meta name="generator" content="Hugo 0.79.0" />

    <script>
        function setTheme() {
            const now = (new Date()).getHours();
            const prev = Number(localStorage.getItem('oldHour'));

            if (prev !== now) {
                localStorage.setItem('oldHour', now);
                if (now > 7 && now < 18) {
                    localStorage.setItem("isDark", "false");
                    document.body.classList.remove('dark');
                } else {
                    localStorage.setItem("isDark", "true");
                    document.body.classList.add('dark');
                }
            } else {
                if (localStorage.getItem('isDark') == "true") {
                    document.body.classList.add('dark');
                } else {
                    document.body.classList.remove('dark');
                }
            }
        }
        function switchTheme() {
            if (localStorage.getItem('isDark') == "true") {
                localStorage.setItem('isDark', "false");
                document.body.classList.remove('dark');
            } else {
                localStorage.setItem('isDark', "true");
                document.body.classList.add('dark');
            }
        }
        setTheme();
    </script>
</head><body class="
                    single
                 "><header class="header">
    <nav class="nav">
        <h1 class="logo"><a href="https://blog.aeinrw.com/">aeinrw</a></h1>
        <ul class="menu">
            <li>
                <a href="/posts/"><i class="iconfont icon-timeline menuicon"></i></a>
            </li>
            <li>
                <a href="/tags/"><i class="iconfont icon-tags menuicon"></i></a>
            </li>
            <li>
                <a href="/categories/"><i class="iconfont icon-categories menuicon"></i></a>
            </li>
            <li>
                <a href="javascript:switchTheme%28%29"><i class="iconfont icon-adjust menuicon"></i></a>
            </li>
        </ul>
    </nav>
</header><main id="content">

<header class="page-header">
    <h1 class="post-title">交叉熵损失函数的梯度</h1>
    <div class="post-meta">December 07, 2020</div>
    <ul class="post-tags">
        <li class="post-tag"><a href="https://blog.aeinrw.com/tags/%E6%95%B0%E5%AD%A6">数学</a></li>
        <li class="post-tag"><a href="https://blog.aeinrw.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a></li>
    </ul>
    
</header>

<article class="post-content">
    <h1 id="1-交叉熵梯度的推导">1. 交叉熵梯度的推导</h1>
<h2 id="11-交叉熵损失函数">1.1 交叉熵损失函数</h2>
<p>$$
L(\theta) = -[y^{T}ln(h_{\theta}(X))+(1-y^{T})ln(1-h_{\theta}(X))]
$$</p>
<p>$x$为单个样本</p>
<p>$$
x_{n \times 1} = \begin{pmatrix}
x_{1}\\<br>
&hellip;\\<br>
x_{n}
\end{pmatrix}
$$</p>
<p>$$
h_{\theta}(x)_{1 \times 1} = \frac{1}{1+e^{-x^{T}\theta}}
$$</p>
<p>$X$为全部样本</p>
<p>$$
X_{m\times n} = \begin{pmatrix}
x^{(1)}_{1} &amp; &hellip; &amp; x^{(1)}_{n}\\<br>
&hellip; &amp; &hellip; &amp; &hellip;\\<br>
x^{(m)}_{1} &amp; &hellip; &amp; x^{(m)}_{n}
\end{pmatrix}_{m\times n}
$$</p>
<p>$$
h_{\theta}(X)_{m\times 1} = \frac{1}{1+e^{-X\theta}}
$$</p>
<h2 id="12-求导工具">1.2 求导工具</h2>
<p>函数的依赖关系</p>
<p>$$
\begin{align}
L_{1\times 1} &amp; = 1_{m\times1}^{T}\times z \\<br>
z_{m\times 1} &amp;= f(y) \\<br>
y_{m\times 1} &amp;= g(x) \\<br>
x_{m\times 1} &amp;= A_{m\times n}\theta_{n\times 1} + b \\<br>
L_{1\times 1} \longleftarrow z_{m\times 1} &amp;\longleftarrow y_{m\times 1} \longleftarrow x_{m\times 1} \longleftarrow \theta_{n\times 1}
\end{align}
$$</p>
<p>求导</p>
<ul>
<li>
<p>$$
\frac{\partial y}{\partial x} = \begin{pmatrix}
\frac{\partial y_{1}}{\partial x_{1}} &amp; 0 &amp; &hellip; &amp; 0\\<br>
0 &amp;  \frac{\partial y_{2}}{\partial x_{2}} &amp; 0 &amp; 0\\<br>
&hellip; &amp; &hellip; &amp; &hellip; &amp; &hellip;\\<br>
0 &amp; 0 &amp; 0 &amp; \frac{\partial y_{m}}{\partial x_{m}}
\end{pmatrix}_{m\times m} = \begin{pmatrix}
g'(x_{1}) &amp; 0 &amp; &hellip; &amp; 0\\<br>
0 &amp; g'(x_{2})  &amp; 0 &amp; 0\\<br>
&hellip; &amp; &hellip; &amp; &hellip; &amp; &hellip;\\<br>
0 &amp; 0 &amp; 0 &amp; g'(x_{m})
\end{pmatrix}_{m\times m} = \Sigma(g'(x)) \\<br>
g'(x) = \begin{pmatrix}
g'(x_{1})\\<br>
&hellip; \\<br>
g'(x_{m})
\end{pmatrix}
$$</p>
</li>
<li>
<p>$$
\begin{align}
\frac{\partial L}{\partial \theta}  &amp;= (\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}\frac{\partial x}{\partial \theta})^{T}\frac{\partial L}{\partial z} \\<br>
&amp;= [\Sigma(f'(y))\times \Sigma(g'(x))\times A]^{T}\times 1_{m\times1} \\<br>
&amp;= A^{T}\times[\Sigma(f'(y))\times \Sigma(g'(x))\times1_{m\times1}] \\<br>
&amp;= A^{T}\times[\Sigma(f'(y)g'(x))\times1_{m\times1}] \\<br>
&amp;= A^{T}\times\begin{pmatrix}
f'(y_{1})g'(x_{1}) &amp; 0 &amp; &hellip; &amp; 0\\<br>
0 &amp; f'(y_{2})g'(x_{2})  &amp; 0 &amp; 0\\<br>
&hellip; &amp; &hellip; &amp; &hellip; &amp; &hellip;\\<br>
0 &amp; 0 &amp; 0 &amp; f'(y_{m})g'(x_{m})
\end{pmatrix}\begin{pmatrix}
1\\<br>
1\\<br>
&hellip;\\<br>
1
\end{pmatrix} \\<br>
&amp;= A^{T} \times\begin{pmatrix}
f'(y_{1})g'(x_{1})\\<br>
f'(y_{2})g'(x_{2})\\<br>
&hellip;\\<br>
f'(y_{m})g'(x_{m})
\end{pmatrix}\\<br>
&amp;=A^{T}\times f'(y)g'(x)
\end{align}
$$</p>
</li>
</ul>
<h2 id="13-求导">1.3 求导</h2>
<p>$$
\begin{align}
\frac{\partial L}{\partial \theta} &amp;= \frac{\partial }{\partial \theta}(-[y^{T}ln\frac{1}{1+e^{-X\theta}}+(1^{T}-y^{T})ln\frac{e^{-X\theta}}{1+e^{-X\theta}}]) \\<br>
&amp;= \frac{\partial}{\partial \theta}(-[-y^{T}ln(1+e^{-X\theta})+(1^{T}-y^{T})(-X\theta)-(1^{T}-y^{T})ln(1+e^{-X\theta})])\\<br>
&amp;= \frac{\partial}{\partial \theta}((1^{T}-y^{T})X\theta+1^T\times ln(1+e^{-X\theta}))\\<br>
&amp;= X^{T}(1-y)+(-X^{T})\frac{1}{1+e^{-X\theta}}e^{-X\theta}\\<br>
&amp;= X^{T}(1-y-\frac{e^{-X\theta}}{1+e^{-X\theta}})\\<br>
&amp;= X^{T}(\frac{1}{1+e^{-X\theta}}-y)\\<br>
&amp;= X^{T}(h_{\theta}(X)-y)
\end{align}
$$</p>

    

    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

    

</article>


        </main><footer class="footer">
    <div>
        
        
        
        <span><a href="https://github.com/aeinrw"><i class="iconfont icon-github"></i></a></span>
        
        
        
        <span><i class="iconfont icon-anjianfengexian"></i></span>
        
        <span><a href="https://www.weibo.com/6064805995/profile"><i class="iconfont icon-tubiaozhizuo-"></i></a></span>
        
        
        
        <span><i class="iconfont icon-anjianfengexian"></i></span>
        
        <span><a href="mailto:aeinrw@gmail.com"><i class="iconfont icon-youxiang"></i></a></span>
        
        
        
        <span><i class="iconfont icon-anjianfengexian"></i></span>
        
        <span><a href="https://www.zhihu.com/people/wei-ran-75-78"><i class="iconfont icon-zhihu"></i></a></span>
        
        
    </div>
    
    <div>
        <span>&copy; 2020 <a href="" ZgotmplZ>aeinrw</a></span>
        <span>&middot;</span>
        <span><a href="http://www.beian.miit.gov.cn/">皖ICP备 20011881号</a>️</span>
        
    </div>
</footer></body>

</html>